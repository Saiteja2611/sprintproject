{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91805011-17b5-4e37-8071-e541d7d1e53a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS Pipeline_Run_Audit_Delta (\n",
    "  SourceType STRING,\n",
    "  FileName STRING,\n",
    "  Status STRING,\n",
    "  StartTime TIMESTAMP,\n",
    "  EndTime TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION 'abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/audit_logs/Pipeline_Run_Audit_Delta';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a466303e-242b-4159-8809-a3cc26186e63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------+---------+--------+-------+\n|      Date|Region| Category|Total_kWh|Peak_kWh|Min_kWh|\n+----------+------+---------+---------+--------+-------+\n|2025-11-01|    UK|  Turbine|    12425|  520.62| 138.83|\n|2025-11-01|    UK|    Panel|    11684|   564.1| 163.89|\n|2025-11-01|    UK|  Battery|     8384|   466.6| 127.89|\n|2025-11-01|    UK|Generator|    11987|  423.68| 156.91|\n|2025-11-01|    EU|  Turbine|     9817|  522.42| 163.45|\n|2025-11-01|    EU|    Panel|    13778|  641.45| 171.85|\n|2025-11-01|    EU|  Battery|     7598|  454.95| 123.65|\n|2025-11-01|    EU|Generator|    10916|   410.9| 145.77|\n|2025-11-01|  APAC|  Turbine|    11978|  459.45| 151.85|\n|2025-11-01|  APAC|    Panel|    12404|   523.1| 135.69|\n|2025-11-01|  APAC|  Battery|    10566|  429.15| 130.44|\n|2025-11-01|  APAC|Generator|     8054|  377.35| 129.69|\n|2025-11-01|    NA|  Turbine|    14590|  471.75| 163.27|\n|2025-11-01|    NA|    Panel|    13735|  517.38| 154.57|\n|2025-11-01|    NA|  Battery|    12124|   518.1| 136.83|\n|2025-11-01|    NA|Generator|     8955|  325.88|  133.7|\n|2025-11-01| LATAM|  Turbine|    10826|  420.65| 124.17|\n|2025-11-01| LATAM|    Panel|    10631|  432.77| 143.87|\n|2025-11-01| LATAM|  Battery|    14722|  592.05| 166.15|\n|2025-11-01| LATAM|Generator|     5596|   325.9|  98.31|\n+----------+------+---------+---------+--------+-------+\nonly showing top 20 rows\n+----------+--------------------+-------+--------------------+--------------------+\n|SourceType|            FileName| Status|           StartTime|             EndTime|\n+----------+--------------------+-------+--------------------+--------------------+\n|    NDJSON|smart_meter_telem...|Success|2025-11-18 04:06:...|2025-11-18 04:06:...|\n|       CSV|consumption_summa...|Success|2025-11-18 04:04:...|2025-11-18 04:04:...|\n|      JSON| asset_registry.json|Success|2025-11-18 04:03:...|2025-11-18 04:03:...|\n|       CSV|maintenance_logs.csv|Success|2025-11-18 04:05:...|2025-11-18 04:05:...|\n|       CSV|    asset_master.csv|Success|2025-11-18 04:02:...|2025-11-18 04:02:...|\n|       CSV|consumption_summa...|Success|2025-11-18 04:10:...|2025-11-18 04:10:...|\n+----------+--------------------+-------+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import explode, col\n",
    "# Step 1: Capture start time\n",
    "start_time = datetime.now()\n",
    "# Step 2: Initialize status\n",
    "status = \"Success\"\n",
    "error_message = \"\"\n",
    " \n",
    "# Step 3: Wrap ingestion logic\n",
    "try:\n",
    "    # Retrieve file name from widget\n",
    "    file_name = dbutils.widgets.get(\"p_file_name\")\n",
    " \n",
    "    # Extract base name without extension\n",
    "    base_name = file_name.split('.')[0]\n",
    " \n",
    "    # Create table name with prefix\n",
    "    table_name = f\"bronze_{base_name}\"\n",
    " \n",
    "    # Define Delta path\n",
    "    delta_path = f\"abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/Bronze/{base_name}_delta\"\n",
    " \n",
    "    # Read CSV and write to Delta\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"multiline\", True) \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .load(f\"abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/raw/{file_name}\")\n",
    " \n",
    "    df.write.mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .save(delta_path)\n",
    " \n",
    "    # Register Delta table\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{delta_path}'\n",
    "    \"\"\")\n",
    " \n",
    "    spark.sql(f\"SELECT * FROM {table_name}\").show()\n",
    " \n",
    "except Exception as e:\n",
    "    status = \"Failed\"\n",
    "    error_message = str(e)\n",
    "finally:\n",
    "    # Step 5: Create audit row\n",
    "    end_time = datetime.now()\n",
    "    audit_row = [Row(\n",
    "        SourceType=\"CSV\",  \n",
    "        FileName=file_name,\n",
    "        Status=status,\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time\n",
    " \n",
    " \n",
    " \n",
    "    )]\n",
    "    audit_df = spark.createDataFrame(audit_row)\n",
    " \n",
    "    # Step 6: Write audit log to Delta\n",
    "    audit_log_path = \"abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/audit_logs/Pipeline_Run_Audit_Delta\"\n",
    "    audit_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .save(audit_log_path)\n",
    " \n",
    "    # Step 7: Register audit table (only once per notebook)\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Pipeline_Run_Audit_Delta\n",
    "        USING DELTA\n",
    "        LOCATION '{audit_log_path}'\n",
    "    \"\"\")\n",
    "    spark.sql(\"SELECT * FROM Pipeline_Run_Audit_Delta\").show()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8214249222217643,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_csv_ingest",
   "widgets": {
    "p_file_name": {
     "currentValue": "consumption_summary.csv",
     "nuid": "737d2e38-0326-48ad-95b8-3a10a95db364",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "consumption_summary.csv",
      "label": "",
      "name": "p_file_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "consumption_summary.csv",
      "label": "",
      "name": "p_file_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}