{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74432650-ea7d-458b-a9b5-bb6633a09dce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+--------------------+----+\n|meter_id|        city|region|           timestamp| kWh|\n+--------+------------+------+--------------------+----+\n| MT00001|  Manchester|    UK|2025-11-13T08:00:00Z|2.42|\n| MT00001|  Manchester|    UK|2025-11-13T08:15:00Z|1.74|\n| MT00001|  Manchester|    UK|2025-11-13T08:30:00Z|2.55|\n| MT00001|  Manchester|    UK|2025-11-13T08:45:00Z|1.81|\n| MT00002|Buenos Aires| LATAM|2025-11-13T08:00:00Z|2.76|\n| MT00002|Buenos Aires| LATAM|2025-11-13T08:15:00Z|2.53|\n| MT00002|Buenos Aires| LATAM|2025-11-13T08:30:00Z|1.91|\n| MT00002|Buenos Aires| LATAM|2025-11-13T08:45:00Z|1.76|\n| MT00003|       Paris|    EU|2025-11-13T08:00:00Z|2.98|\n| MT00003|       Paris|    EU|2025-11-13T08:15:00Z|2.81|\n| MT00003|       Paris|    EU|2025-11-13T08:30:00Z|1.68|\n| MT00003|       Paris|    EU|2025-11-13T08:45:00Z|2.26|\n| MT00004|   S達o Paulo| LATAM|2025-11-13T08:00:00Z|1.64|\n| MT00004|   S達o Paulo| LATAM|2025-11-13T08:15:00Z|2.43|\n| MT00004|   S達o Paulo| LATAM|2025-11-13T08:30:00Z|2.66|\n| MT00004|   S達o Paulo| LATAM|2025-11-13T08:45:00Z|2.28|\n| MT00005|Buenos Aires| LATAM|2025-11-13T08:00:00Z|2.31|\n| MT00005|Buenos Aires| LATAM|2025-11-13T08:15:00Z|2.87|\n| MT00005|Buenos Aires| LATAM|2025-11-13T08:30:00Z|2.15|\n| MT00005|Buenos Aires| LATAM|2025-11-13T08:45:00Z| 2.5|\n+--------+------------+------+--------------------+----+\nonly showing top 20 rows\n+----------+--------------------+-------+--------------------+--------------------+\n|SourceType|            FileName| Status|           StartTime|             EndTime|\n+----------+--------------------+-------+--------------------+--------------------+\n|    NDJSON|smart_meter_telem...|Success|2025-11-18 04:06:...|2025-11-18 04:06:...|\n|       CSV|consumption_summa...|Success|2025-11-18 04:04:...|2025-11-18 04:04:...|\n|    NDJSON|smart_meter_telem...|Success|2025-11-18 04:10:...|2025-11-18 04:11:...|\n|      JSON| asset_registry.json|Success|2025-11-18 04:03:...|2025-11-18 04:03:...|\n|       CSV|maintenance_logs.csv|Success|2025-11-18 04:05:...|2025-11-18 04:05:...|\n|       CSV|    asset_master.csv|Success|2025-11-18 04:02:...|2025-11-18 04:02:...|\n|       CSV|consumption_summa...|Success|2025-11-18 04:10:...|2025-11-18 04:10:...|\n|      JSON| asset_registry.json|Success|2025-11-18 04:10:...|2025-11-18 04:10:...|\n+----------+--------------------+-------+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import explode, col\n",
    " \n",
    "# -------------------------------\n",
    "# Step 1: Capture Start Time\n",
    "# -------------------------------\n",
    "start_time = datetime.now()\n",
    "status = \"Success\"\n",
    "error_message = \"\"\n",
    " \n",
    "# -------------------------------\n",
    "# Step 2: Try Block for Ingestion\n",
    "# -------------------------------\n",
    "try:\n",
    "    # Retrieve Parameters\n",
    "    file_name = dbutils.widgets.get(\"p_file_name\")\n",
    "    base_name = file_name.split('.')[0]\n",
    "    table_name = f\"bronze_{base_name}\"\n",
    " \n",
    "    # Define Paths\n",
    "    input_path = \"abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/raw\"\n",
    "    checkpoint_path = f\"abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/_checkpoint/{base_name}\"\n",
    "    output_path = f\"abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/Bronze/{base_name}_delta\"\n",
    "    schema_path = f\"abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/schema_location/{base_name}\"\n",
    " \n",
    "    # Read Stream with Auto Loader\n",
    "    df = (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "            .option(\"cloudFiles.schemaHints\", \"meter_id STRING, location STRUCT<city: STRING, region: STRING>, readings ARRAY<STRUCT<kWh: DOUBLE, timestamp: STRING>>\")\n",
    "            .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "            .load(input_path)\n",
    "    )\n",
    " \n",
    "    # Flatten Nested JSON\n",
    "    df_flat = df.select(\n",
    "        col(\"meter_id\"),\n",
    "        col(\"location.city\").alias(\"city\"),\n",
    "        col(\"location.region\").alias(\"region\"),\n",
    "        explode(\"readings\").alias(\"reading\")\n",
    "    ).select(\n",
    "        \"meter_id\", \"city\", \"region\",\n",
    "        col(\"reading.timestamp\").alias(\"timestamp\"),\n",
    "        col(\"reading.kWh\").alias(\"kWh\")\n",
    "    )\n",
    " \n",
    "    # Write to Bronze Delta Table\n",
    "    query = (\n",
    "        df_flat.writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(availableNow=True)\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .option(\"checkpointLocation\", checkpoint_path)\n",
    "            .start(output_path)\n",
    "    )\n",
    "    query.awaitTermination()\n",
    " \n",
    "    # Register Delta Table\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    spark.sql(f\"CREATE TABLE {table_name} USING DELTA LOCATION '{output_path}'\")\n",
    "    spark.sql(f\"SELECT * FROM {table_name}\").show()\n",
    " \n",
    "except Exception as e:\n",
    "    status = \"Failed\"\n",
    "    error_message = str(e)\n",
    "    print(f\"Ingestion failed: {error_message}\")\n",
    "    raise\n",
    " \n",
    "# -------------------------------\n",
    "# Step 3: Audit Logging\n",
    "# -------------------------------\n",
    "finally:\n",
    "    end_time = datetime.now()\n",
    " \n",
    "    audit_row = [Row(\n",
    "        SourceType=\"NDJSON\",\n",
    "        FileName=file_name,\n",
    "        Status=status,\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time\n",
    "    )]\n",
    "    audit_df = spark.createDataFrame(audit_row)\n",
    " \n",
    "    # Write audit log to Delta\n",
    "    audit_log_path = \"abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/audit_logs/Pipeline_Run_Audit_Delta\"\n",
    "    audit_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .save(audit_log_path)\n",
    " \n",
    "    # Register audit table (if not already registered)\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Pipeline_Run_Audit_Delta\n",
    "        USING DELTA\n",
    "        LOCATION '{audit_log_path}'\n",
    "    \"\"\")\n",
    " \n",
    "    # Optional: Preview audit log\n",
    "    spark.sql(\"SELECT * FROM Pipeline_Run_Audit_Delta\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8764203402630935,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_nestedjson_ingest",
   "widgets": {
    "p_file_name": {
     "currentValue": "smart_meter_telemetry.ndjson",
     "nuid": "ce9f9550-ccdf-481e-8b46-61e4fc638479",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "smart_meter_telemetry.ndjson",
      "label": "",
      "name": "p_file_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "smart_meter_telemetry.ndjson",
      "label": "",
      "name": "p_file_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}