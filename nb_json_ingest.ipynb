{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e418cc2e-d376-416b-a137-9dcc8d4ccf3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------+------------+-------+------+\n|asset_id| category|commissioned|manufacturer|  model|region|\n+--------+---------+------------+------------+-------+------+\n|       1|  Turbine|  2018-11-19|          GE| TX-525|    EU|\n|       2|  Battery|  2022-11-05|          GE| PN-180| LATAM|\n|       3|Generator|  2018-06-05|     Siemens| GE-497|    NA|\n|       4|  Turbine|  2021-10-15|     LG Chem|LGX-667|    EU|\n|       5|  Turbine|  2022-10-09|   SolarEdge| PN-261|    NA|\n|       6|Generator|  2020-03-18|     LG Chem| PN-716|    EU|\n|       7|  Battery|  2020-04-09|     LG Chem|LGX-593| LATAM|\n|       8|  Turbine|  2020-06-09|     LG Chem| SE-195|    UK|\n|       9|  Turbine|  2018-12-21|     LG Chem| PN-563|    UK|\n|      10|  Turbine|  2019-08-20|     Siemens|LGX-762|    NA|\n|      11|  Turbine|  2021-03-02|     Siemens|LGX-156|    UK|\n|      12|Generator|  2019-05-29|   Panasonic|LGX-755|    EU|\n|      13|Generator|  2018-06-18|          GE| SE-311|    UK|\n|      14|Generator|  2021-12-20|   Panasonic| TX-511|    NA|\n|      15|    Panel|  2023-05-05|   Panasonic| PN-496|  APAC|\n|      16|  Turbine|  2018-11-12|   Panasonic| SE-444| LATAM|\n|      17|  Battery|  2022-11-25|   SolarEdge| PN-988| LATAM|\n|      18|  Turbine|  2022-05-14|   Panasonic| SE-759|    NA|\n|      19|Generator|  2022-05-20|     LG Chem| TX-364|    NA|\n|      20|  Battery|  2019-09-20|   SolarEdge| SE-962|    UK|\n+--------+---------+------------+------------+-------+------+\nonly showing top 20 rows\n+----------+--------------------+-------+--------------------+--------------------+\n|SourceType|            FileName| Status|           StartTime|             EndTime|\n+----------+--------------------+-------+--------------------+--------------------+\n|    NDJSON|smart_meter_telem...|Success|2025-11-18 04:06:...|2025-11-18 04:06:...|\n|       CSV|consumption_summa...|Success|2025-11-18 04:04:...|2025-11-18 04:04:...|\n|      JSON| asset_registry.json|Success|2025-11-18 04:03:...|2025-11-18 04:03:...|\n|       CSV|maintenance_logs.csv|Success|2025-11-18 04:05:...|2025-11-18 04:05:...|\n|       CSV|    asset_master.csv|Success|2025-11-18 04:02:...|2025-11-18 04:02:...|\n|       CSV|consumption_summa...|Success|2025-11-18 04:10:...|2025-11-18 04:10:...|\n|      JSON| asset_registry.json|Success|2025-11-18 04:10:...|2025-11-18 04:10:...|\n+----------+--------------------+-------+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    " \n",
    "# Step 1: Capture start time\n",
    "start_time = datetime.now()\n",
    " \n",
    "# Step 2: Initialize status\n",
    "status = \"Success\"\n",
    "error_message = \"\"\n",
    " \n",
    "# Step 3: Wrap ingestion logic\n",
    "try:\n",
    "    file_name = dbutils.widgets.get(\"p_file_name\")\n",
    "    # Extract base name without extension\n",
    "    base_name = file_name.split('.')[0]\n",
    "    # Create table name with prefix\n",
    "    table_name = f\"bronze_{base_name}\"\n",
    "    # Define Delta path\n",
    "    delta_path = f\"abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/Bronze/{file_name}_delta\"\n",
    "    # Read CSV and write to Delta\n",
    "    df = spark.read.format(\"json\") \\\n",
    "        .option(\"multiline\", True) \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .load(f'abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/raw/{file_name}')\n",
    "    df.write.mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .save(delta_path)\n",
    "        # Register Delta table\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '{delta_path}'\"\"\")\n",
    "    # Query the table\n",
    "    spark.sql(f\"SELECT * FROM {table_name}\").show()\n",
    "except Exception as e:\n",
    "    status = \"Failed\"\n",
    "    error_message = str(e)\n",
    "    print(f\"Ingestion failed: {error_message}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Step 4: Capture end time\n",
    "    end_time = datetime.now()\n",
    " \n",
    "    # Step 5: Create audit row\n",
    "    audit_row = [Row(\n",
    "        SourceType=\"JSON\",\n",
    "        FileName=file_name,\n",
    "        Status=status,\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time\n",
    "    )]\n",
    "    audit_df = spark.createDataFrame(audit_row)\n",
    "    # Step 6: Write audit log to Delta\n",
    "    audit_log_path = \"abfss://sedpcontainer@sedpstorageaccount.dfs.core.windows.net/audit_logs/Pipeline_Run_Audit_Delta\"\n",
    "    audit_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .save(audit_log_path)\n",
    " \n",
    "    # Step 7: Register audit table (only once per notebook)\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Pipeline_Run_Audit_Delta\n",
    "        USING DELTA\n",
    "        LOCATION '{audit_log_path}'\n",
    "    \"\"\")\n",
    "    spark.sql(\"SELECT * FROM Pipeline_Run_Audit_Delta\").show()\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e00bc1a-af7a-434b-a0d6-f9a618e631d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7857475211011779,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_json_ingest",
   "widgets": {
    "p_file_name": {
     "currentValue": "asset_registry.json",
     "nuid": "2db5997d-7ec2-4e3c-b447-93419868bc53",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "asset_registry.json",
      "label": "",
      "name": "p_file_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "asset_registry.json",
      "label": "",
      "name": "p_file_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}